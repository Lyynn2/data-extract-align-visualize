# -*- coding: utf-8 -*-
"""01 Bulk Data Download from AWS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OUpB69dFSVuYq68fyLvOPgCY0yuMxbSu

# AWS DATA Access using Python
This notebook is a tutorial on programmatically accessing AWS S3 files using Python

### **Step 1: Set up AWS credentials**

*   Request admin for an IAM user with appropriate S3 access permissions.
*   Obtain the Access Key ID and Secret Access Key for programmatic access. Note, the admin will need to appropriately set up and provide the Access Key ID and Secret Access Key which is different from the console access from a browser.

### **Step 2: Install and Import the Required Libraries**

First, install the boto3 Python library to interact with AWS S3 using python
"""

# !pip install boto3

# import boto3 to interact with S3
import boto3

# install other libraries as needed
import datetime
import pandas as pd
import os

"""### **Step 4: Configure S3 Client & Resource Objects**"""

from boto3 import session

# note : please don't expose these credentials anywhere. For long term use, please request David for new credentials for each user
session = boto3.Session(
    aws_access_key_id='AKIA2EVNV6T3N6RNEBUX',
    aws_secret_access_key='84VitjLsq2i686J+WFuErtJqSuXLonUJzLu83v+d'
)

# session = boto3.Session(
#     aws_access_key_id='YOUR_ACCESS_KEY',
#     aws_secret_access_key='YOUR_SECRET_ACCESS_KEY'
# )

# Configure the bucket-name and main directory in S3
bucket_name = 'ceti-data'
prefix = "raw/"

# Configure the S3 client and resource object
s3_client = session.client('s3')
s3_resource = session.resource('s3')

# Get the paginator for the S3 client to load all the pages of results
paginator = s3_client.get_paginator('list_objects_v2')

# Get the result iterator to iterate through each page of results
result_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)


#####################################################
#####################################################


"""## **Helper functions for s3 download operations**

### **Download all files from a specific folder or subfolder**

Get the path to the location of the folder or subfolder and assign it to the prefix parameter
"""

def download_files_from_folder(s3_client, bucket_name, source_path, dest_path,
                               extensions_to_include=None,
                               extensions_to_exclude=None):
    """
    Download files from a specific folder or subfolder in an S3 bucket to the local filesystem.

    Args:
        s3_client (boto3.client): The S3 client object.
        bucket_name (str): The name of the S3 bucket.
        source_path (str): The prefix or source directory path within the bucket.
        dest_path (str): The local path where the files will be saved.
    """
    
    # Sanitize any provided extensions.
    if extensions_to_include is not None:
      extensions_to_include = [x.lower().strip('.').strip() for x in extensions_to_include]
    if extensions_to_exclude is not None:
      extensions_to_exclude = [x.lower().strip('.').strip() for x in extensions_to_exclude]
    
    # Create S3 objects.
    paginator = s3_client.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=source_path)
    
    # Find files to download.
    files_to_download = []
    file_sizes_to_download_mb = []
    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            key_size_mb = obj['Size']/1024/1024
            # Apply specified filters based on file extension.
            extension = os.path.splitext(key)[-1].strip('.').lower()
            if extensions_to_exclude is not None and extension in extensions_to_exclude:
                continue
            if extensions_to_include is not None and extension not in extensions_to_include:
                continue
            local_file_path = os.path.join(dest_path, os.path.relpath(key, source_path))
            # Add the file to download.
            if not key.endswith('/'): # not a folder
                files_to_download.append((bucket_name, key, local_file_path))
                file_sizes_to_download_mb.append(key_size_mb)
    
    # Print a summary of files to download.
    num_files = len(files_to_download)
    max_filename_length = max([len(key.replace(source_path, '')) for (bucket_name, key, _) in files_to_download])
    total_size_mb = sum(file_sizes_to_download_mb)
    print('Downloading %d files (%d MB) from %s/%s to %s' % (num_files, total_size_mb,
                                                             bucket_name, source_path, dest_path))
    if extensions_to_include is not None:
      print('  Only included extensions %s' % str(extensions_to_include))
    if extensions_to_exclude is not None:
      print('  Excluded extensions %s' % str(extensions_to_exclude))

    # Specify how often to print download status.
    # Will currently print before every file.
    # If your console doesn't support overwriting text by printing \r,
    #   and you want less frequent printouts,
    #   then you could set the below to something like 1/20.
    print_status_increment_fraction = 1/num_files
    # Download the files!
    for (file_index, (bucket_name, key, local_file_path)) in enumerate(files_to_download):
        # Print a status update if desired.
        if file_index % max(1, int(num_files*print_status_increment_fraction)) == 0 or file_index == num_files-1:
            filename = key.replace(source_path, '')
            filename_padded = filename.ljust(max_filename_length)
            downloaded_size_mb = sum(file_sizes_to_download_mb[0:(file_index+1)])
            print('\r  > Downloaded %d/%d MB (%0.1f%%) | file %*d/%d | current %s' % (
              downloaded_size_mb, total_size_mb, 100*downloaded_size_mb/total_size_mb,
              len(str(num_files)), file_index+1, num_files, filename_padded), end='')
        # Check if the file has already been downloaded.
        if os.path.exists(local_file_path):
            continue
        # Download the file!
        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)
        s3_client.download_file(bucket_name, key, local_file_path)
        # print(f"File downloaded: s3://{bucket_name}/{key} -> {local_file_path}")
    # Clear the status printout line.
    print('\r', ' '*max_filename_length, ' '*50, ' '*(len(str(num_files)*2)), end='')
    

"""### **Download a single file from s3**"""

# Download a specific file from s3 using its file key (path inside the the main bucket. Eg: raw/2022-08-04/wt-b827eba0694e/1657553673181.flac)
def download_file_from_s3(s3_client, bucket_name, key, local_path):
    """
    Download a file from S3 to the local filesystem.

    Args:
        bucket_name (str): The name of the S3 bucket.
        key (str): The key of the object in the S3 bucket.
        local_path (str): The local path where the file will be saved.
    """
    local_file_path = os.path.join(local_path, os.path.basename(key))
    s3_client.download_file(bucket_name, key, local_file_path)
    print(f"File downloaded: s3://{bucket_name}/{key} -> {local_file_path}")

#####################################################

# # define the file key for the file you want to download
# key = 'raw/2022-08-04/wt-b827eba0694e/1657553673181.flac'


# # define the local path to store the file
# local_path = os.getcwd()
#
#
# # download the file to the provided local path path
# download_file_from_s3(s3_client, bucket_name, key, local_path)

#####################################################

### Downloading files from a sub-folder

# provide path to the particular sub-folder location in s3. Eg. 'raw/2022-08-04/wt-ccda5bea'
# source_path = 'raw/2022-08-04/wt-b827eba0694e'
source_path = 'raw/2023-07-08'

# provide the local path or a new foldername to create in the current working directory
dest_path = 'path_to_data_root_folder'
assert dest_path != 'path_to_data_root_folder', 'Please remember to update the variable "data_root_dir"'

download_files_from_folder(s3_client, bucket_name, source_path, dest_path,
                           extensions_to_include=None, # None to not filter, or a list of extensions
                           extensions_to_exclude=['mp4']) # None to not filter, or a list of extensions

# Note that I excluded MP4 files since I plan on just using the LRF files for now.

# # Example of only downloading images:
# download_files_from_folder(s3_client, bucket_name, source_path, dest_path,
#                            extensions_to_include=['jpg'], # None to not filter, or a list of extensions
#                            extensions_to_exclude=None) # None to not filter, or a list of extensions

#####################################################







# """### **Generate presigned URLs to open sensor data or play media files without downloading it**"""
#
# # generate presigned url for a specific file
# def generate_presigned_url(s3_client, bucket_name, object_key, expiration=604800):
#     """
#     Generate a pre-signed URL for accessing an object in an S3 bucket.
#
#     Args:
#         s3_client (boto3.client): The S3 client object.
#         bucket_name (str): The name of the S3 bucket.
#         object_key (str): The key of the object in the S3 bucket.
#         expiration (int): The expiration time in seconds for the pre-signed URL.
#
#     Returns:
#         str: The pre-signed URL.
#     """
#     url = s3_client.generate_presigned_url(
#         'get_object',
#         Params={'Bucket': bucket_name, 'Key': object_key},
#         ExpiresIn=expiration
#     )
#     return url
#
# ### Generate presigned urls for each file ina a specific folder
# def generate_presigned_urls_for_folder(s3_client, bucket_name, source_path, expiration=604800):
#     """
#     Generate pre-signed URLs for all the files in a folder path in an S3 bucket.
#
#     Args:
#         s3_client (boto3.client): The S3 client object.
#         bucket_name (str): The name of the S3 bucket.
#         source_path (str): The prefix or folder path within the bucket.
#         expiration (int): The expiration time in seconds for the pre-signed URLs.
#
#     Returns:
#         dict: A dictionary mapping file keys to their corresponding pre-signed URLs.
#     """
#     s3_client = boto3.client('s3')
#     paginator = s3_client.get_paginator('list_objects_v2')
#     page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=source_path)
#
#     presigned_urls = {}
#     for page in page_iterator:
#         for obj in page.get('Contents', []):
#                 key = obj['Key']
#                 url = generate_presigned_url(bucket_name, key, expiration)
#                 presigned_urls[key] = url
#
#     return presigned_urls

# """### **Play Media Files Using Presigned URL**"""
#
# !pip install smart_open
# !apt install -y ffmpeg
# !pip install pydub librosa
# !apt install -y ffmpeg
#
# """### Playing a flac-encoded audio file"""
#
# import smart_open
# from IPython.display import Audio
#
#
# # Generate the pre-signed URL
# key = 'raw/2023-07-07/wt-98d5d13f/1688355546819.flac'
# url = generate_presigned_url(s3_client, bucket_name, key, expiration=604800)
# print(f"url generated: {url}")
#
#
# # Open the audio file
# with smart_open.open(url, 'rb') as audio_file:
#     # Create an Audio object and play the audio
#     audio = Audio(data=audio_file.read())
#     display(audio)

